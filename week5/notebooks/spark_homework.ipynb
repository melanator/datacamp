{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2bddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2edf4a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502f33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 18:47:52 WARN Utils: Your hostname, DESKTOP-LIPMSCN resolves to a loopback address: 127.0.1.1; using 172.27.8.77 instead (on interface eth0)\n",
      "22/03/01 18:47:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/anton/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 18:48:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/01 18:48:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e274fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f60a42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv('head.csv')\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e9d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7336efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c09adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62889a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('data/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "903cf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d53635c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 19:05:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/01 19:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/01 19:05:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/01 19:05:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('data/fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe0eb205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('data/fhvhv/2021/01/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34ba8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e509599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02765|2021-02-03 07:11:34|2021-02-03 07:25:03|         197|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-04 17:39:53|2021-02-04 18:00:26|          36|          80|   null|\n",
      "|           HV0003|              B02764|2021-02-01 17:14:46|2021-02-01 17:19:17|          86|          86|   null|\n",
      "|           HV0003|              B02764|2021-02-05 04:28:58|2021-02-05 04:49:17|          80|         173|   null|\n",
      "|           HV0003|              B02869|2021-02-04 17:33:01|2021-02-04 17:44:37|          80|          37|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd550955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78e6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('trip_duration', df.dropoff_datetime - df.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abf21cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+--------------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|       trip_duration|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+--------------------+\n",
      "|           HV0003|              B02765|2021-02-03 07:11:34|2021-02-03 07:25:03|         197|          76|   null|13 minutes 29 sec...|\n",
      "|           HV0005|              B02510|2021-02-04 17:39:53|2021-02-04 18:00:26|          36|          80|   null|20 minutes 33 sec...|\n",
      "|           HV0003|              B02764|2021-02-01 17:14:46|2021-02-01 17:19:17|          86|          86|   null|4 minutes 31 seconds|\n",
      "|           HV0003|              B02764|2021-02-05 04:28:58|2021-02-05 04:49:17|          80|         173|   null|20 minutes 19 sec...|\n",
      "|           HV0003|              B02869|2021-02-04 17:33:01|2021-02-04 17:44:37|          80|          37|   null|11 minutes 36 sec...|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61a808ea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not parse datatype: interval",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_duration\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mselect(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickup_datetime\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-02-15\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py:1399\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \n\u001b[1;32m   1396\u001b[0m \u001b[38;5;124;03m    >>> df.select(df.age).collect()\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;124;03m    [Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m:\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1402\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py:1032\u001b[0m, in \u001b[0;36mDataFrame.columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolumns\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns all column names as a list.\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m    >>> df.columns\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m    ['age', 'name']\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241m.\u001b[39mfields]\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py:256\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_datatype_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to parse datatype from schema. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e)\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:871\u001b[0m, in \u001b[0;36m_parse_datatype_json_string\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_datatype_json_string\u001b[39m(json_string):\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;124;03m\"\"\"Parses the given data type JSON string.\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    >>> import pickle\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    >>> def check_datatype(datatype):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    >>> check_datatype(complex_maptype)\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_datatype_json_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:888\u001b[0m, in \u001b[0;36m_parse_datatype_json_value\u001b[0;34m(json_value)\u001b[0m\n\u001b[1;32m    886\u001b[0m tpe \u001b[38;5;241m=\u001b[39m json_value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpe \u001b[38;5;129;01min\u001b[39;00m _all_complex_types:\n\u001b[0;32m--> 888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_all_complex_types\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tpe \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mudt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDefinedType\u001b[38;5;241m.\u001b[39mfromJson(json_value)\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:579\u001b[0m, in \u001b[0;36mStructType.fromJson\u001b[0;34m(cls, json)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromJson\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json):\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructType([StructField\u001b[38;5;241m.\u001b[39mfromJson(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:579\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromJson\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json):\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructType([\u001b[43mStructField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:436\u001b[0m, in \u001b[0;36mStructField.fromJson\u001b[0;34m(cls, json)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromJson\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json):\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructField(json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m--> 436\u001b[0m                        \u001b[43m_parse_datatype_json_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    437\u001b[0m                        json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnullable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    438\u001b[0m                        json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/types.py:884\u001b[0m, in \u001b[0;36m_parse_datatype_json_value\u001b[0;34m(json_value)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DecimalType(\u001b[38;5;28mint\u001b[39m(m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28mint\u001b[39m(m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 884\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse datatype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m json_value)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     tpe \u001b[38;5;241m=\u001b[39m json_value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse datatype: interval"
     ]
    }
   ],
   "source": [
    "df.select(['pickup_datetime', 'trip_duration']).select(df.pickup_datetime == \"2021-02-15\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8de18e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(*) \n",
    "from trips \n",
    "where pickup_datetime >= '2021-02-15 00:00:00' and \n",
    "pickup_datetime <= '2021-02-15 23:59:59'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "545d03eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|pickup_datetime|duration|\n",
      "+---------------+--------+\n",
      "|     2021-02-11|      20|\n",
      "|     2021-02-17|      15|\n",
      "|     2021-02-20|      12|\n",
      "|     2021-02-03|      11|\n",
      "|     2021-02-19|      10|\n",
      "|     2021-02-25|       9|\n",
      "|     2021-02-18|       9|\n",
      "|     2021-02-10|       9|\n",
      "|     2021-02-25|       9|\n",
      "|     2021-02-10|       9|\n",
      "|     2021-02-18|       9|\n",
      "|     2021-02-20|       9|\n",
      "|     2021-02-05|       8|\n",
      "|     2021-02-09|       8|\n",
      "|     2021-02-10|       8|\n",
      "|     2021-02-03|       8|\n",
      "|     2021-02-08|       8|\n",
      "|     2021-02-10|       8|\n",
      "|     2021-02-02|       8|\n",
      "|     2021-02-21|       8|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:=============================>                            (4 + 4) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date(pickup_datetime), extract(hour from (dropoff_datetime-pickup_datetime)) as duration \n",
    "from trips\n",
    "order by 2 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e942f4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|dispatching_base_num|count(1)|\n",
      "+--------------------+--------+\n",
      "|              B02510| 3233664|\n",
      "|              B02764|  965568|\n",
      "|              B02872|  882689|\n",
      "|              B02875|  685390|\n",
      "|              B02765|  559768|\n",
      "|              B02869|  429720|\n",
      "|              B02887|  322331|\n",
      "|              B02871|  312364|\n",
      "|              B02864|  311603|\n",
      "|              B02866|  311089|\n",
      "|              B02878|  305185|\n",
      "|              B02682|  303255|\n",
      "|              B02617|  274510|\n",
      "|              B02883|  251617|\n",
      "|              B02884|  244963|\n",
      "|              B02882|  232173|\n",
      "|              B02876|  215693|\n",
      "|              B02879|  210137|\n",
      "|              B02867|  200530|\n",
      "|              B02877|  198938|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT dispatching_base_num, count(*) from trips\n",
    "group by dispatching_base_num\n",
    "order by 2 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35f8b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------+\n",
      "|PULocationID|DOLocationID|count(1)|\n",
      "+------------+------------+--------+\n",
      "|         132|         265|   12542|\n",
      "|         188|          61|   11814|\n",
      "|          36|          37|   11491|\n",
      "|          37|          36|   11487|\n",
      "|          61|         188|   11462|\n",
      "|          61|         225|   11342|\n",
      "|         225|          61|   11293|\n",
      "|          35|          76|   11244|\n",
      "|          39|          76|   10688|\n",
      "|          76|          35|   10621|\n",
      "|          42|          41|   10304|\n",
      "|          41|          42|   10260|\n",
      "|          17|          61|   10185|\n",
      "|          61|          17|   10181|\n",
      "|         259|         265|   10091|\n",
      "|          77|          76|    9748|\n",
      "|           7|         223|    9465|\n",
      "|          76|          39|    9338|\n",
      "|         215|         130|    9288|\n",
      "|          76|          77|    9162|\n",
      "+------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT PULocationID, DOLocationID, count(*) from trips\n",
    "where PULocationID != DOLocationID\n",
    "group by PULocationID, DOLocationID\n",
    "order by 3 desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
